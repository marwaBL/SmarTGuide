{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers import Dense, Lambda, Activation, LSTM, Reshape, Conv1D, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten, concatenate, RepeatVector, multiply\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop, Nadam\n",
    "from keras.regularizers import l2\n",
    "from Dataset import Dataset\n",
    "from evaluate import evaluate_model\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import gc\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run MCRec.\")\n",
    "    parser.add_argument('--dataset', nargs='?', default='ml-100k',\n",
    "                        help='Choose a dataset.')\n",
    "    parser.add_argument('--epochs', type=int, default=30,\n",
    "                        help='Number of epochs.')\n",
    "    parser.add_argument('--batch_size', type=int, default=256,\n",
    "                        help='Batch size.')\n",
    "    parser.add_argument('--learner', nargs='?', default='adam',\n",
    "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
    "    parser.add_argument('--verbose', type=int, default=1,\n",
    "                        help='Show performance per X iterations')\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--latent_dim', type=int, default='128',\n",
    "                        help=\"Embedding size for user and item embedding\")\n",
    "    parser.add_argument('--latent_layer_dim', nargs='?', default='[512, 256, 128, 64]',\n",
    "                        help=\"Embedding size for each layer\")\n",
    "    parser.add_argument('--num_neg', type=int, default=4,\n",
    "                        help='Number of negative instances to pair with a positive instance.')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "def slice(x, index):\n",
    "    return x[:, index, :, :]\n",
    "\n",
    "\n",
    "def slice_2(x, index):\n",
    "    return x[:, index, :]\n",
    "\n",
    "def path_attention(user_latent, item_latent, path_latent, latent_size, att_size, path_attention_layer_1, path_attention_layer_2, path_name):\n",
    "    #user_latent (batch_size, latent_size)\n",
    "    #item_latent (batch_size, latent_size)\n",
    "    #path_latent (batch_size, path_num, mp_latent_size)\n",
    "    latent_size = user_latent.shape[1].value\n",
    "    path_num, path_latent_size = path_latent.shape[1].value, path_latent.shape[2].value\n",
    "\n",
    "    path = Lambda(slice_2, output_shape=(path_latent_size,), arguments={'index':0})(path_latent)\n",
    "    inputs = concatenate([user_latent, item_latent, path])\n",
    "    output = (path_attention_layer_1(inputs))\n",
    "    output = (path_attention_layer_2(output))\n",
    "    for i in range(1, path_num):\n",
    "        path = Lambda(slice_2, output_shape=(path_latent_size,), arguments={'index':i})(path_latent)\n",
    "        inputs = concatenate([user_latent, item_latent, path])\n",
    "        tmp_output = (path_attention_layer_1(inputs))\n",
    "        tmp_output = (path_attention_layer_2(tmp_output))\n",
    "        output = concatenate([output, tmp_output])\n",
    "\n",
    "\n",
    "    atten = Lambda(lambda x : K.softmax(x), name = '%s_attention_softmax'%path_name)(output)\n",
    "    output = Lambda(lambda x: K.sum(x[0] * K.expand_dims(x[1], -1), 1))([path_latent, atten])\n",
    "    return output\n",
    "\n",
    "def get_umtmum_embedding(umtmum_input, path_num, timestamps, length, user_latent, item_latent, path_attention_layer_1, path_attention_layer_2):\n",
    "    conv_umtmum = Conv1D(filters = 128,\n",
    "                       kernel_size = 4,\n",
    "                       activation = 'relu',\n",
    "                       kernel_regularizer = l2(0.0),\n",
    "                       kernel_initializer = 'glorot_uniform',\n",
    "                       padding = 'valid',\n",
    "                       strides = 1,\n",
    "                       name = 'umtmum_conv')\n",
    "\n",
    "    path_input = Lambda(slice, output_shape=(timestamps, length), arguments={'index':0})(umtmum_input)\n",
    "    output = conv_umtmum(path_input)\n",
    "    output = GlobalMaxPooling1D()(output)\n",
    "    output = Dropout(0.5)(output)\n",
    "\n",
    "    for i in range(1, path_num):\n",
    "        path_input = Lambda(slice, output_shape=(timestamps, length), arguments={'index':i})(umtmum_input)\n",
    "        tmp_output = GlobalMaxPooling1D()(conv_umtmum(path_input))\n",
    "        tmp_output = Dropout(0.5)(tmp_output)\n",
    "        output = concatenate([output, tmp_output])\n",
    "\n",
    "    output = Reshape((path_num, 128))(output)\n",
    "    #output = path_attention(user_latent, item_latent, output, 128, 64, path_attention_layer_1, path_attention_layer_2, 'umtmum')\n",
    "    output = GlobalMaxPooling1D()(output)\n",
    "    return output\n",
    "\n",
    "def get_umtm_embedding(umtm_input, path_num, timestamps, length, user_latent, item_latent, path_attention_layer_1, path_attention_layer_2):\n",
    "    conv_umtm = Conv1D(filters = 128,\n",
    "                       kernel_size = 4,\n",
    "                       activation = 'relu',\n",
    "                       kernel_regularizer = l2(0.0),\n",
    "                       kernel_initializer = 'glorot_uniform',\n",
    "                       padding = 'valid',\n",
    "                       strides = 1,\n",
    "                       name = 'umtm_conv')\n",
    "\n",
    "    path_input = Lambda(slice, output_shape=(timestamps, length), arguments={'index':0})(umtm_input)\n",
    "    output = GlobalMaxPooling1D()(conv_umtm(path_input))\n",
    "    output = Dropout(0.5)(output)\n",
    "\n",
    "    for i in range(1, path_num):\n",
    "        path_input = Lambda(slice, output_shape=(timestamps, length), arguments={'index':i})(umtm_input)\n",
    "        tmp_output = GlobalMaxPooling1D()(conv_umtm(path_input))\n",
    "        tmp_output = Dropout(0.5)(tmp_output)\n",
    "        output = concatenate([output, tmp_output])\n",
    "\n",
    "    output = Reshape((path_num, 128))(output)\n",
    "    #output = path_attention(user_latent, item_latent, output, 128, 64, path_attention_layer_1, path_attention_layer_2, 'umtm')\n",
    "    output = GlobalMaxPooling1D()(output)\n",
    "    return output\n",
    "\n",
    "def get_umum_embedding(umum_input, path_num, timestamps, length, user_latent, item_latent, path_attention_layer_1, path_attention_layer_2):\n",
    "    conv_umum = Conv1D(filters = 128,\n",
    "                       kernel_size = 4,\n",
    "                       activation = 'relu',\n",
    "                       kernel_regularizer = l2(0.0),\n",
    "                       kernel_initializer = 'glorot_uniform',\n",
    "                       padding = 'valid',\n",
    "                       strides = 1,\n",
    "                       name = 'umum_conv')\n",
    "\n",
    "    path_input = Lambda(slice, output_shape=(timestamps, length), arguments={'index':0})(umum_input)\n",
    "    output = GlobalMaxPooling1D()(conv_umum(path_input))\n",
    "    output = Dropout(0.5)(output)\n",
    "\n",
    "    for i in range(1, path_num):\n",
    "        path_input = Lambda(slice, output_shape=(timestamps, length), arguments={'index':i})(umum_input)\n",
    "        tmp_output = GlobalMaxPooling1D()(conv_umum(path_input))\n",
    "        tmp_output = Dropout(0.5)(tmp_output)\n",
    "        output = concatenate([output, tmp_output])\n",
    "\n",
    "\n",
    "    output = Reshape((path_num, 128))(output)\n",
    "    #output = path_attention(user_latent, item_latent, output, 128, 64, path_attention_layer_1, path_attention_layer_2, 'umum')\n",
    "    output = GlobalMaxPooling1D()(output)\n",
    "    return output\n",
    "\n",
    "def get_uuum_embedding(umum_input, path_num, timestamps, length, user_latent, item_latent, path_attention_layer_1, path_attention_layer_2):\n",
    "    conv_umum = Conv1D(filters = 128,\n",
    "                       kernel_size = 4,\n",
    "                       activation = 'relu',\n",
    "                       kernel_regularizer = l2(0.0),\n",
    "                       kernel_initializer = 'glorot_uniform',\n",
    "                       padding = 'valid',\n",
    "                       strides = 1,\n",
    "                       name = 'uuum_conv')\n",
    "\n",
    "    path_input = Lambda(slice, output_shape=(timestamps, length), arguments={'index':0})(umum_input)\n",
    "    output = GlobalMaxPooling1D()(conv_umum(path_input))\n",
    "    output = Dropout(0.5)(output)\n",
    "\n",
    "    for i in range(1, path_num):\n",
    "        path_input = Lambda(slice, output_shape=(timestamps, length), arguments={'index':i})(umum_input)\n",
    "        tmp_output = GlobalMaxPooling1D()(conv_umum(path_input))\n",
    "        tmp_output = Dropout(0.5)(tmp_output)\n",
    "        output = concatenate([output, tmp_output])\n",
    "\n",
    "\n",
    "    output = Reshape((path_num, 128))(output)\n",
    "    #output = path_attention(user_latent, item_latent, output, 128, 64, path_attention_layer_1, path_attention_layer_2, 'uuum')\n",
    "    output = GlobalMaxPooling1D()(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def metapath_attention(user_latent, item_latent, metapath_latent, latent_size, att_size):\n",
    "    #user_latent (batch_size, latent_size)\n",
    "    #item_latent (batch_size, latent_size)\n",
    "    #metapath_latent (batch_size, path_num, mp_latent_size)\n",
    "    #print user_latent.shape\n",
    "    latent_size = user_latent.shape[1].value\n",
    "    path_num, mp_latent_size = metapath_latent.shape[1].value, metapath_latent.shape[2].value\n",
    "    dense_layer_1 = Dense(att_size,\n",
    "                        activation = 'relu',\n",
    "                        kernel_initializer = 'glorot_normal',\n",
    "                        kernel_regularizer = l2(0.001),\n",
    "                        name = 'metapath_attention_layer_1')\n",
    "\n",
    "    dense_layer_2 = Dense(1,\n",
    "                          activation = 'relu',\n",
    "                          kernel_initializer = 'glorot_normal',\n",
    "                          kernel_regularizer =l2(0.001),\n",
    "                          name = 'metapath_attention_layer_2')\n",
    "\n",
    "    metapath = Lambda(slice_2, output_shape=(mp_latent_size,), arguments={'index':0})(metapath_latent)\n",
    "    inputs = concatenate([user_latent, item_latent, metapath])\n",
    "    output = (dense_layer_1(inputs))\n",
    "    output = (dense_layer_2(output))\n",
    "    for i in range(1, path_num):\n",
    "        metapath = Lambda(slice_2, output_shape=(mp_latent_size,), arguments={'index':i})(metapath_latent)\n",
    "        inputs = concatenate([user_latent, item_latent, metapath])\n",
    "        tmp_output = (dense_layer_1(inputs))\n",
    "        tmp_output = (dense_layer_2(tmp_output))\n",
    "        output = concatenate([output, tmp_output])\n",
    "\n",
    "\n",
    "    atten = Lambda(lambda x : K.softmax(x), name = 'metapath_attention_softmax')(output)\n",
    "    output = Lambda(lambda x: K.sum(x[0] * K.expand_dims(x[1], -1), 1))([metapath_latent, atten])\n",
    "    return output\n",
    "\n",
    "\n",
    "def user_attention(user_latent, path_output):\n",
    "    latent_size = user_latent.shape[1].value\n",
    "\n",
    "    inputs = concatenate([user_latent, path_output])\n",
    "    output = Dense(latent_size,\n",
    "                   activation = 'relu',\n",
    "                   kernel_initializer = 'glorot_normal',\n",
    "                   kernel_regularizer =l2(0.001),\n",
    "                   name = 'user_attention_layer')(inputs)\n",
    "    atten = Lambda(lambda x : K.softmax(x), name = 'user_attention_softmax')(output)\n",
    "    output = multiply([user_latent, atten])\n",
    "    return output\n",
    "\n",
    "def item_attention(item_latent, path_output):\n",
    "    latent_size = item_latent.shape[1].value\n",
    "\n",
    "    inputs = concatenate([item_latent, path_output])\n",
    "    output = Dense(latent_size,\n",
    "                   activation = 'relu',\n",
    "                   kernel_initializer = 'glorot_normal',\n",
    "                   kernel_regularizer =l2(0.001),\n",
    "                   name = 'item_attention_layer')(inputs)\n",
    "    atten = Lambda(lambda x : K.softmax(x), name = 'item_attention_softmax')(output)\n",
    "    output = multiply([item_latent, atten])\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_model(usize, isize, path_nums, timestamps, length, layers = [20, 10], reg_layers = [0, 0], latent_dim = 40, reg_latent = 0):\n",
    "    user_input = Input(shape = (1,), dtype = 'int32', name = 'user_input', sparse = False)\n",
    "    item_input = Input(shape = (1,), dtype = 'int32', name = 'item_input', sparse = False)\n",
    "    umtm_input = Input(shape = (path_nums[0], timestamps[0], length,), dtype = 'float32', name = 'umtm_input')\n",
    "    umum_input = Input(shape = (path_nums[1], timestamps[1], length,), dtype = 'float32', name = 'umum_input')\n",
    "    umtmum_input = Input(shape = (path_nums[2], timestamps[2], length,), dtype = 'float32', name = 'umtmum_input')\n",
    "    uuum_input = Input(shape = (path_nums[3], timestamps[3], length, ), dtype = 'float32', name = 'uuum_input')\n",
    "    Embedding_User_Feedback = Embedding(input_dim = usize,\n",
    "                                    output_dim = latent_dim,\n",
    "                                    input_length = 1,\n",
    "                                    embeddings_initializer = 'glorot_normal',\n",
    "                                    name = 'user_feedback_embedding')\n",
    "\n",
    "    Embedding_Item_Feedback = Embedding(input_dim = isize,\n",
    "                                    output_dim = latent_dim,\n",
    "                                    input_length = 1,\n",
    "                                    embeddings_initializer = 'glorot_normal',\n",
    "                                    name = 'item_feedback_embedding')\n",
    "    user_latent = Reshape((latent_dim,))(Flatten()(Embedding_User_Feedback(user_input)))\n",
    "    item_latent = Reshape((latent_dim,))(Flatten()(Embedding_Item_Feedback(item_input)))\n",
    "\n",
    "\n",
    "    path_attention_layer_1 = Dense(128,\n",
    "                                   activation = 'relu',\n",
    "                                   kernel_regularizer = l2(0.001),\n",
    "                                   kernel_initializer = 'glorot_normal',\n",
    "                                   name = 'path_attention_layer_1')\n",
    "\n",
    "    path_attention_layer_2 = Dense(1,\n",
    "                                   activation = 'relu',\n",
    "                                   kernel_regularizer = l2(0.001),\n",
    "                                   kernel_initializer = 'glorot_normal',\n",
    "                                   name = 'path_attention_layer_2')\n",
    "\n",
    "    umtm_latent = get_umtm_embedding(umtm_input, path_nums[0], timestamps[0], length, user_latent, item_latent, path_attention_layer_1, path_attention_layer_2)\n",
    "    umum_latent = get_umum_embedding(umum_input, path_nums[1], timestamps[1], length, user_latent, item_latent, path_attention_layer_1, path_attention_layer_2)\n",
    "    umtmum_latent = get_umtmum_embedding(umtmum_input, path_nums[2], timestamps[2], length, user_latent, item_latent, path_attention_layer_1, path_attention_layer_2)\n",
    "    uuum_latent = get_uuum_embedding(uuum_input, path_nums[3], timestamps[3], length, user_latent, item_latent, path_attention_layer_1, path_attention_layer_2)\n",
    "\n",
    "    path_output = concatenate([umtm_latent, umum_latent, umtmum_latent, uuum_latent])\n",
    "    path_output = Reshape((4, 128))(path_output)\n",
    "    path_output = metapath_attention(user_latent, item_latent, path_output, latent_dim, 128)\n",
    "\n",
    "    user_atten = user_attention(user_latent, path_output)\n",
    "    item_atten = item_attention(item_latent, path_output)\n",
    "\n",
    "    output = concatenate([user_atten, path_output, item_atten])\n",
    "    for idx in range(0, len(layers)):\n",
    "        layer = Dense(layers[idx],\n",
    "                      kernel_regularizer = l2(0.001),\n",
    "                      kernel_initializer = 'glorot_normal',\n",
    "                      activation = 'relu',\n",
    "                      name = 'item_layer%d' % idx)\n",
    "        output = layer(output)\n",
    "\n",
    "    #user_output = concatenate([user_atten, path_output])\n",
    "    #for idx in xrange(0, len(layers)):\n",
    "    #    layer = Dense(layers[idx],\n",
    "    #                  kernel_regularizer = l2(0.001),\n",
    "    #                  kernel_initializer = 'glorot_normal',\n",
    "    #                  activation = 'relu',\n",
    "    #                  name = 'user_layer%d' % idx)\n",
    "    #    user_output = layer(user_output)\n",
    "\n",
    "    #item_output = concatenate([path_output, item_atten])\n",
    "    #for idx in xrange(0, len(layers)):\n",
    "    #    layer = Dense(layers[idx],\n",
    "    #                  kernel_regularizer = l2(0.001),\n",
    "    #                  kernel_initializer = 'glorot_normal',\n",
    "    #                  activation = 'relu',\n",
    "    #                  name = 'item_layer%d' % idx)\n",
    "    #item_output = layer(item_output)\n",
    "\n",
    "    #output = concatenate([user_output, item_output])\n",
    "\n",
    "    print( 'output.shape = ', output.shape)\n",
    "    prediction_layer = Dense(1,\n",
    "                       activation = 'sigmoid',\n",
    "                       kernel_initializer = 'lecun_normal',\n",
    "                       name = 'prediction')\n",
    "\n",
    "    prediction = prediction_layer(output)\n",
    "    model = Model(inputs = [user_input, item_input, umtm_input, umum_input, umtmum_input, uuum_input], outputs = [prediction])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_train_instances(user_feature, item_feature, type_feature, path_umtm, path_umum, path_umtmum, path_uuum, path_nums, timestamps, train_list, num_negatives, batch_size, shuffle = True):\n",
    "    num_batches_per_epoch = int((len(train_list) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(train_list)\n",
    "        while True:\n",
    "            if shuffle == True:\n",
    "                np.random.shuffle(train_list)\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                k = 0\n",
    "                _user_input = np.zeros((batch_size * (num_negatives + 1),))\n",
    "                _item_input = np.zeros((batch_size * (num_negatives + 1),))\n",
    "                _umtm_input = np.zeros((batch_size * (num_negatives + 1), path_nums[0], timestamps[0], 64))\n",
    "                _umum_input = np.zeros((batch_size * (num_negatives + 1), path_nums[1], timestamps[1], 64))\n",
    "                _umtmum_input = np.zeros((batch_size * (num_negatives + 1), path_nums[2], timestamps[2], 64))\n",
    "                _uuum_input = np.zeros((batch_size * (num_negatives + 1), path_nums[3], timestamps[3], 64))\n",
    "                _labels = np.zeros(batch_size * (num_negatives + 1))\n",
    "\n",
    "                for u, i in train_list[start_index : end_index]:\n",
    "\n",
    "                    _user_input[k] = u\n",
    "                    _item_input[k] = i\n",
    "\n",
    "                    \n",
    "\n",
    "                    if (u, i) in path_umum:\n",
    "                        for p_i in range(len(path_umum[(u, i)])):\n",
    "                            for p_j in range(len(path_umum[(u, i)][p_i])):\n",
    "                                type_id = path_umum[(u, i)][p_i][p_j][0]\n",
    "                                index = path_umum[(u, i)][p_i][p_j][1]\n",
    "                                if type_id == 1 :\n",
    "                                    _umum_input[k][p_i][p_j] = user_feature[index]\n",
    "                                elif type_id == 2 :\n",
    "                                    _umum_input[k][p_i][p_j] = item_feature[index]\n",
    "                                elif type_id == 3 :\n",
    "                                    _umum_input[k][p_i][p_j] = type_feature[index]\n",
    "\n",
    "                    if (u, i) in path_umtmum:\n",
    "                        for p_i in range(len(path_umtmum[(u, i)])):\n",
    "                            for p_j in range(len(path_umtmum[(u, i)][p_i])):\n",
    "                                type_id = path_umtmum[(u, i)][p_i][p_j][0]\n",
    "                                index = path_umtmum[(u, i)][p_i][p_j][1]\n",
    "                                if type_id == 1 :\n",
    "                                    _umtmum_input[k][p_i][p_j] = user_feature[index]\n",
    "                                elif type_id == 2 :\n",
    "                                    _umtmum_input[k][p_i][p_j] = item_feature[index]\n",
    "                                elif type_id == 3 :\n",
    "                                    _umtmum_input[k][p_i][p_j] = type_feature[index]\n",
    "\n",
    "                    if (u, i) in path_uuum:\n",
    "                        for p_i in range(len(path_uuum[(u, i)])):\n",
    "                            for p_j in range(len(path_uuum[(u, i)][p_i])):\n",
    "                                type_id = path_uuum[(u, i)][p_i][p_j][0]\n",
    "                                index = path_uuum[(u, i)][p_i][p_j][1]\n",
    "                                if type_id == 1 :\n",
    "                                    _uuum_input[k][p_i][p_j] = user_feature[index]\n",
    "                                elif type_id == 2 :\n",
    "                                    _uuum_input[k][p_i][p_j] = item_feature[index]\n",
    "                                elif type_id == 3 :\n",
    "                                    _uuum_input[k][p_i][p_j] = type_feature[index]\n",
    "                    _labels[k] = 1.0\n",
    "                    k += 1\n",
    "                    #negative instances\n",
    "                    for t in range(num_negatives):\n",
    "                        j = np.random.randint(1, num_items-1)\n",
    "                        while j in user_item_map[u]:\n",
    "                            j = np.random.randint(1, num_items-1)\n",
    "\n",
    "                        _user_input[k] = u\n",
    "                        _item_input[k] = j\n",
    "\n",
    "                        if (u, j) in path_umtm:\n",
    "                            for p_i in range(len(path_umtm[(u, j)])):\n",
    "                                for p_j in range(len(path_umtm[(u, j)][p_i])):\n",
    "                                    type_id = path_umtm[(u, j)][p_i][p_j][0]\n",
    "                                    index = path_umtm[(u, j)][p_i][p_j][1]\n",
    "                                    if type_id == 1 :\n",
    "                                        _umtm_input[k][p_i][p_j] = user_feature[index]\n",
    "                                    elif type_id == 2 :\n",
    "                                        _umtm_input[k][p_i][p_j] = item_feature[index]\n",
    "                                    elif type_id == 3 :\n",
    "                                        _umtm_input[k][p_i][p_j] = type_feature[index]\n",
    "\n",
    "                        if (u, j) in path_umum:\n",
    "                            for p_i in range(len(path_umum[(u, j)])):\n",
    "                                for p_j in range(len(path_umum[(u, j)][p_i])):\n",
    "                                    type_id = path_umum[(u, j)][p_i][p_j][0]\n",
    "                                    index = path_umum[(u, j)][p_i][p_j][1]\n",
    "                                    if type_id == 1 :\n",
    "                                        _umum_input[k][p_i][p_j] = user_feature[index]\n",
    "                                    elif type_id == 2 :\n",
    "                                        _umum_input[k][p_i][p_j] = item_feature[index]\n",
    "                                    elif type_id == 3 :\n",
    "                                        _umum_input[k][p_i][p_j] = type_feature[index]\n",
    "                        if (u, j) in path_umtmum:\n",
    "                            for p_i in range(len(path_umtmum[(u, j)])):\n",
    "                                for p_j in range(len(path_umtmum[(u, j)][p_i])):\n",
    "                                    type_id = path_umtmum[(u, j)][p_i][p_j][0];\n",
    "                                    index = path_umtmum[(u, j)][p_i][p_j][1]\n",
    "                                    if type_id == 1 :\n",
    "                                        _umtmum_input[k][p_i][p_j] = user_feature[index]\n",
    "                                    elif type_id == 2 :\n",
    "                                        _umtmum_input[k][p_i][p_j] = item_feature[index]\n",
    "                                    elif type_id == 3 :\n",
    "                                        _umtmum_input[k][p_i][p_j] = type_feature[index]\n",
    "\n",
    "                        \n",
    "                        _labels[k] = 0.0\n",
    "                        k += 1\n",
    "                yield ([_user_input, _item_input, _umtm_input, _umum_input, _umtmum_input, _uuum_input], _labels)\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    dataset = args.dataset\n",
    "    latent_dim = args.latent_dim\n",
    "    layers = eval(args.latent_layer_dim)\n",
    "    learning_rate = args.lr\n",
    "    epochs = args.epochs\n",
    "    batch_size = args.batch_size\n",
    "    num_negatives = args.num_neg\n",
    "    learner = args.learner\n",
    "    verbose = args.verbose\n",
    "\n",
    "    # print (\"layers : \",layers, \"  type : \", type(layers))\n",
    "\n",
    "    out = 0\n",
    "    reg_latent = 0\n",
    "    reg_layes = [0 ,0, 0, 0]\n",
    "    evaluation_threads = 1\n",
    "    topK = 10\n",
    "\n",
    "    # dataset = 'ml-100k'\n",
    "    # latent_dim = 128\n",
    "    # reg_latent = 0\n",
    "    # layers = [512, 256, 128, 64]\n",
    "    # reg_layes = [0 ,0, 0, 0]\n",
    "    # learning_rate = 0.001\n",
    "    # epochs = 30\n",
    "    # batch_size = 256\n",
    "    # num_negatives = 4\n",
    "    # learner = 'adam'\n",
    "    # verbose = 1\n",
    "    # out = 0\n",
    "\n",
    "\n",
    "    print('num_negatives = ', num_negatives)\n",
    "\n",
    "    t1 = time()\n",
    "    dataset = Dataset('../data/' + dataset)\n",
    "    trainMatrix, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    train = dataset.train\n",
    "    user_item_map = dataset.user_item_map\n",
    "    item_user_map = dataset.item_user_map\n",
    "    path_umtm = dataset.path_umtm\n",
    "    path_umum = dataset.path_umum\n",
    "    path_umtmum = dataset.path_umtmum\n",
    "    path_uuum = dataset.path_uuum\n",
    "    user_feature, item_feature, type_feature = dataset.user_feature, dataset.item_feature, dataset.type_feature\n",
    "    num_users, num_items = trainMatrix.shape[0], trainMatrix.shape[1]\n",
    "    path_nums = [dataset.umtm_path_num, dataset.umum_path_num, dataset.umtmum_path_num, dataset.uuum_path_num]\n",
    "    timestamps = [dataset.umtm_timestamp, dataset.umum_timestamp, dataset.umtmum_timestamp, dataset.uuum_timestamp]\n",
    "    length = dataset.fea_size\n",
    "\n",
    "    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" % (time()-t1, num_users, num_items, len(train), len(testRatings)))\n",
    "    print( 'path nums = ', path_nums)\n",
    "    print( 'timestamps = ', timestamps)\n",
    "\n",
    "    model = get_model(num_users, num_items, path_nums, timestamps, length, layers, reg_layes, latent_dim, reg_latent)\n",
    "    model.compile(optimizer = Adam(lr = learning_rate, decay = 1e-4),\n",
    "                  loss = 'binary_crossentropy')\n",
    "    #model.compile(optimizer = Nadam(),\n",
    "    #              loss = 'binary_crossentropy')\n",
    "\n",
    "\n",
    "    # Check Init performance\n",
    "    t1 = time()\n",
    "    (ps, rs, ndcgs) = evaluate_model(model, user_feature, item_feature, type_feature, num_users, num_items, path_umtm, path_umum, path_umtmum, path_uuum, path_nums, timestamps, length, testRatings, testNegatives, topK, evaluation_threads)\n",
    "    p, r, ndcg = np.array(ps).mean(), np.array(rs).mean(), np.array(ndcgs).mean()\n",
    "    print('Init: Precision = %.4f, Recall = %.4f, NDCG = %.4f [%.1f]' %(p, r, ndcg, time()-t1))\n",
    "\n",
    "    best_p = -1\n",
    "    p_list, r_list, ndcg_list = [], [], []\n",
    "    print( 'Begin training....')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t1 = time()\n",
    "\n",
    "        #Generate training instance\n",
    "        train_steps, train_batches = get_train_instances(user_feature, item_feature, type_feature, path_umtm, path_umum, path_umtmum, path_uuum, path_nums, timestamps, train, num_negatives, batch_size, True)\n",
    "        t = time()\n",
    "        print( '[%.1f s] epoch %d train_steps %d' % (t - t1, epoch, train_steps))\n",
    "        #Training\n",
    "        hist = model.fit_generator(train_batches,\n",
    "                                   train_steps,\n",
    "                                   epochs = 1,\n",
    "                                   verbose = 0)\n",
    "        print( 'training time %.1f s' % (time() - t))\n",
    "\n",
    "\n",
    "\n",
    "        t2 = time()\n",
    "        if epoch % verbose == 0:\n",
    "            (ps, rs, ndcgs) = evaluate_model(model, user_feature, item_feature, type_feature, num_users, num_items, path_umtm, path_umum, path_umtmum, path_uuum, path_nums, timestamps, length, testRatings, testNegatives, topK, evaluation_threads)\n",
    "            p, r, ndcg, loss = np.array(ps).mean(), np.array(rs).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print('Iteration %d [%.1f s]: Precision = %.4f, Recall = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]'\n",
    "                  % (epoch,  t2-t1, p, r, ndcg, loss, time()-t2))\n",
    "\n",
    "            #if p > best_p:\n",
    "            #    best_p = p\n",
    "            #    attention_layer_model = Model(inputs=model.input,\n",
    "            #                          outputs = [model.get_layer('user_input').output, model.get_layer('item_input').output, model.get_layer('metapath_attention_softmax').output])\n",
    "            #    [user_input_output, item_input_output, metapath_attention_output] = attention_layer_model.predict_generator(train_batches, train_steps)\n",
    "            #    with open('../data/ml-100k.attention_2', 'w') as outfile:\n",
    "            #        num = user_input_output.shape[0]\n",
    "            #        for i in range(num):\n",
    "            #            outfile.write(str(user_input_output[i]) + ',' + str(item_input_output[i]))\n",
    "            #            for j in range(metapath_attention_output.shape[1]):\n",
    "            #                outfile.write(' ' + str(metapath_attention_output[i][j]))\n",
    "            #            outfile.write('\\n')\n",
    "            #    print 'write succeccfully...'\n",
    "            p_list.append(p)\n",
    "            r_list.append(r)\n",
    "            ndcg_list.append(ndcg)\n",
    "    print(\"End. Precision = %.4f, Recall = %.4f, NDCG = %.4f. \" %(max(p_list), max(r_list), max(ndcg_list)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
